# Чат-бот для отбора на интенсив от Тинькофф в Сириусе

## Небольшая документация
Использованные модели: `tinkoff-ai/ruDialoGPT-small`, `tinkoff-ai/ruDialoGPT-medium`, `ruT5-base`
Веса моделей: [Ссылка](https://drive.google.com/drive/folders/1VNivVZygJDDMCP9ALHss6lUoDD7oBaTg?usp=sharing)

Процесс обучения:
1. Экспортировала данные из Telegram-чатов и распарсила JSON (уже был дан скрипт).
2. В качестве данных для обучения брала только те сообщения, у которых есть 3-4 реплая, чтобы лучше сохранялся контекст. А также только те, длина которых была <500 символов, чтобы вмещалось в модель (эмпирически найденное число).
3. Так как GPT-2 - авторегрессионная модель (т.е. та, которая учится предсказывать следующий токен на основе предыдущих), использовала входные данные и как inputs, и как targets.

Пояснения: 
- в датасете ~ 200к сэмплов (3 контекста и 1 ответ), но т.к. время на решение задачи ограничено, а я не обладаю мощными вычислительными ресурсами, брала только те сэмплы, у которых все контексты и ответ были непустыми. Это также помогает модели лучше улавливать схожесть сообщений в диалоге.
- по-хорошему при инференсе нужно генерировать текст, учитывая предыдущие контексты, однако в моем случае модель почему-то отправляла один и тот же текст (хорошая тема на подумать!), поэтому решено было убрать контексты и генерировать ответы только по последнему сообщению пользователя.
- модель на одни и те же вопросы генерирует одни и те же ответы (это связано с маленьким набором данных для обучения (см. п.1)). Это проблема, но если немного поменять вопрос, ответ будет другим.

## Обзоры на модели
### 1. tinkoff-ai/ruDialoGPT-small
> *Время обучения: 1.5 часа (~10 минут/эпоха, 9 эпох без переобучения)*

На вопросы, куда лучше пойти стажироваться, модель ответила так:

```
> посоветуй, куда пойти стажироваться студенту пмишнику
> в мак

> куда лучше пойти на стажировку - в сбер или в яндекс?
> в яндекс легко

> куда лучше пойти на стажировку - вк или яндекс?
> лучше в яндекс идти стажировка 10 часов

> куда лучше пойти на стажировку: яндекс или тинькофф?
> тинькофф пон

> куда лучше пойти на стажировку: вк или сбер?
> сбер, лучше вообще не нырять в сливы, если не умеешь добывать коралл (пояснение от автора: скорее всего, это какое-то подобие народной мудрости :) )
```

### 2. tinkoff-ai/ruDialoGPT-medium
> *Время обучения: 2 часа (~20 минут/эпоха, 6 эпох без переобучения)*

В этот раз примеры будут из разных сфер:

```
> скажи, почему я так нравлюсь [отредактирвано автором: на самом деле тут было имя человека]?
> потому что ты золотце❤️
Удивительно, что теперь модель может присылать смайлики (хотя скорее всего это произошло, т.к. я увеличила датасет)

> Какие твои самые любимые аниме?
> Гибсонг, тетрадь смерти, тетрадь смерти 2, тетрадь смерти 3

> как попасть на стажировку?
> нужно просто зарешать катбуст

> Продолжи песню: ночь, что за странная свобода
> Я вышел на улицу, я встретил рассвет

> в чем смысл жизни?
> Смысла нет, просто живи
```

### 3. T5-base
> *Время обучения: 3 часа (~60 минут/эпоха, 3 эпохи без переобучения)*

С этой моделью ничего не получилось, т.к. по времени было очень долго. После второй эпохи генерировалась какая-то белиберда, т.е. модель скорее всего не обучилась.

## Планы на будущее
1. Разобраться с генерацией по контексту
2. Поэкспериментировать с T5-base и другими моделями

## Как запустить?

1. Установите Docker (узнать как это сделать можно, загуглив в интернете).
2. В консоли пропишите `docker pull misshimichka/tinkoff-ml:1`.
3. После этого пропишите `docker run -e BOT_TOKEN=сюда_вставьте_токен_своего_бота misshimichka/tinkoff-ml:1`.
4. Тыкайте!